<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.32.3" />
  <meta name="author" content="Boyuan Jiang">
  <meta name="description" content="Graduate of Artificial Intelligence">

  
  <link rel="alternate" hreflang="en-us" href="https://byjiang.com/blog/DNN-Step-By-Step/">

  
  


  

  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="/libs/academicons/1.8.1/css/academicons.min.css">
  <link rel="stylesheet" href="/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113237942-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="/libs/autotrack/2.4.1/autotrack.js"></script>
    
  

  
  <link rel="alternate" href="https://byjiang.com/index.xml" type="application/rss+xml" title="进击的加菲猫">
  <link rel="feed" href="https://byjiang.com/index.xml" type="application/rss+xml" title="进击的加菲猫">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://byjiang.com/blog/DNN-Step-By-Step/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="进击的加菲猫">
  <meta property="og:url" content="https://byjiang.com/blog/DNN-Step-By-Step/">
  <meta property="og:title" content="从零开始使用Python写一个深度神经网络 | 进击的加菲猫">
  <meta property="og:description" content="">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-09-27T00:33:08&#43;08:00">
  
  <meta property="article:modified_time" content="2017-09-27T00:33:08&#43;08:00">
  

  

  <title>从零开始使用Python写一个深度神经网络 | 进击的加菲猫</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">进击的加菲猫</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/blog/">
            
            <span>Blog</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#news">
            
            <span>News</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">从零开始使用Python写一个深度神经网络</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-09-27 00:33:08 &#43;0800 CST" itemprop="datePublished dateModified">
      Sep 27, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Boyuan Jiang">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8Python%e5%86%99%e4%b8%80%e4%b8%aa%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;url=https%3a%2f%2fbyjiang.com%2fblog%2fDNN-Step-By-Step%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbyjiang.com%2fblog%2fDNN-Step-By-Step%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbyjiang.com%2fblog%2fDNN-Step-By-Step%2f&amp;title=%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8Python%e5%86%99%e4%b8%80%e4%b8%aa%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fbyjiang.com%2fblog%2fDNN-Step-By-Step%2f&amp;title=%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8Python%e5%86%99%e4%b8%80%e4%b8%aa%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8Python%e5%86%99%e4%b8%80%e4%b8%aa%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;body=https%3a%2f%2fbyjiang.com%2fblog%2fDNN-Step-By-Step%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        <p>这篇文章主要基于Andrew Ng的deeplearning course中week4的编程作业，基于python实现一个可以判断图片中是否有猫的深度神经网络，输出一个介于[0,1]之间的概率值。在具体的实现过程中，我们不会使用任何现成的深度学习框架，仅使用numpy库提供的必要的数学运算函数，实现前向传播、反向传播、梯度更新等操作。
手工实现一遍神经网络的每一个细节，对于我们了解其背后的理论知识有很大的帮助。
</p>

<h2 id="基础知识">基础知识</h2>

<p>在文章的开始，我们首先简单的回顾一下实现一个神经网络所需要的数学知识。</p>

<h3 id="什么是神经网络">什么是神经网络</h3>

<p>简单地说，一个神经网络就是由多个网络层，每层都包含多个神经元互相全连接所构成的网络。如下图1所示，上半部分表示了一个神经元的构成，输入的x与神经元的参数W，b计算得到$z=W^Tx+b$然后在经过激活函数激活后得到输出$a=\sigma(z)$。常见的激活函数有sigmoid、relu、tanh等。图一的下半部分则是一个两层的神经网络示意图，第一层有三个神经元、第二层有一个神经元，对于本文提到的二分类问题，最终的输出是一个介于0,1之间的概率值。
<img src="http://oq393r0ea.bkt.clouddn.com/20170924234619.png" alt="" />
<center>图1 神经网络示意图</center></p>

<h3 id="前向传播-forward-propagation">前向传播(Forward propagation)</h3>

<p>构建完神经网络后，首先需要从左往右依次计算每个神经元的输出，直至网络中的最后一个神经元输出一个图片分类的概率值。这一过程就是前向传播的过程。如下图2所示，对于我们刚才提到的简单的2层神经网络，可以利用右边方框中的四个式子完成整个前向传播的计算过程。相对与一会要介绍的反向传播、前向传播还是比较容易理解和实现的。
<img src="http://oq393r0ea.bkt.clouddn.com/20170925000950.png" alt="" />
<center>图2 前向传播示意图示意图</center></p>

<h3 id="损失函数-cost-function">损失函数(Cost function)</h3>

<p>在对网络进行反向传播之前，我们首先需要对前向传播计算的输出和样本的真实值之间的误差计算损失，对于本文所介绍的二分类的例子，常用的损失函数为交叉熵损失(cross-entropy cost)，关于交叉熵的详细定义及用在二分类问题上的优势可以参考我之前写的一篇文章*<a href="http://www.aiboy.pub/2017/07/16/Cross_Entropy_Cost_Function/" target="_blank">为什么选择交叉熵(Cross Entropy)代价函数</a>*。</p>

<h3 id="反向传播-backward-propagation">反向传播(Backward propagation)</h3>

<p>反向传播是神经网络得以训练和优化的重要环节，反向传播实质上就是利用<a href="https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99" target="_blank">链式法则(chain rule)</a>求解每个参数相对于损失函数的偏导的过程。下图3是单层(第$l$层)神经网络反向传播计算$dZ^{[l]},dW^{[l]},db^{[l]},dA^{[l-1]}$的公式。$dZ$是激活前神经元的输出值的偏导，可以看做来自后层的导数$dA^{[l]}$与该神经元的激活函数的导数$g^{[l]&lsquo;}(Z^{[l]})$相乘(链式法则)。又因为$Z=WX+b$(这里的$X$是上层神经元输出的激活值$A^{[l-1]}$)，所以有$dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}$,$db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdim=True)$。注意，这里求解的过程中都除以样本数m是因为在实际训练中，我们会用到向量化的技术，一次训练一批(m个)样本。最后计算$dA^{[l-1]}=W^{[l]T}dZ^{[l]}$用于给更上一层神经元计算梯度使用。
<img src="http://oq393r0ea.bkt.clouddn.com/20170925103032.jpg" alt="" />
<center>图3 单层反向传播示意图</center></p>

<h3 id="梯度更新">梯度更新</h3>

<p>完成了反向传播后，我们就已经得到了神经元中各个参数的梯度信息，这时我们就可以使用梯度下降算法对参数进行更新。关于常见的梯度下降算法的介绍以及优缺点，可以参考我的<a href="http://www.aiboy.pub/2017/09/10/A_Brief_Of_Optimization_Algorithms/" target="_blank">这篇博文</a>。在这里，我们只使用最简单的<a href="http://www.aiboy.pub/2017/09/10/A_Brief_Of_Optimization_Algorithms/#sgd" target="_blank">梯度下降算法</a>。</p>

<h3 id="小结">小结</h3>

<p>至此，我们已经介绍了实现一个神经网络所需要的理论知识，包括神经元的构成、前向传播、损失函数计算、反向传播、参数更新等部分。在本文的第二部分我们将只使用python+numpy一步步编程实现以上所说的各部分。Let&rsquo;s go!</p>

<h2 id="python实现神经网络">Python实现神经网络</h2>

<h3 id="helper-function">helper function</h3>

<p>首先，让我们实现一些简单的helper function，这些函数的构建可以简化之后的编程过程。</p>

<h4 id="sigmoid">sigmoid</h4>

<p>sigmoid是一种常见的激活函数，一般用于多层神经网络的输出层，用于将输出值放缩到[0,1]之间。sigmoid的计算如式(1)所示：
$$A=\frac{1}{1+e^{-Z}}  \tag{1} $$</p>

<pre><code class="language-python">def sigmoid(Z):
    &quot;&quot;&quot;
    Implements the sigmoid activation in numpy
    
    Arguments:
    Z -- numpy array of any shape
    
    Returns:
    A -- output of sigmoid(z), same shape as Z
    cache -- returns Z as well, useful during backpropagation
    &quot;&quot;&quot;
    
    A = 1/(1+np.exp(-Z))
    cache = Z
    
    return A, cache
</code></pre>

<p>对(1)式求导，可以得到：
$$A&rsquo;=A(1-A)  \tag{2}$$
因此，sigmoid的反向传播可以由以下代码实现：</p>

<pre><code class="language-python">def sigmoid_backward(dA, cache):
    &quot;&quot;&quot;
    Implement the backward propagation for a single SIGMOID unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    &quot;&quot;&quot;
    
    Z = cache
    
    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)
    
    assert (dZ.shape == Z.shape)
    
    return dZ
</code></pre>

<p>其中cache是在前向传播中保存的神经元激活前的Z值，这样我们可以很方便的得到激活后的值A。</p>

<h4 id="relu">relu</h4>

<p>relu也是一种常见的激活函数，定义为：
$$A=max(0,Z)  \tag{3}$$
与sigmoid激活相比，它在大于0的部分斜率恒为1，因此不会出现sigmoid的饱和情况，即对于sigmoid激活输出Z很大时，反传时的梯度接近于0，导致网络的学习很慢。</p>

<pre><code class="language-python">def relu(Z):
    &quot;&quot;&quot;
    Implement the RELU function.

    Arguments:
    Z -- Output of the linear layer, of any shape

    Returns:
    A -- Post-activation parameter, of the same shape as Z
    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently
    &quot;&quot;&quot;
    
    A = np.maximum(0,Z)
    
    assert(A.shape == Z.shape)
    
    cache = Z 
    return A, cache
</code></pre>

<p>relu的方向传播也很简单，只需把小于0部分梯度至0.</p>

<pre><code class="language-python">def relu_backward(dA, cache):
    &quot;&quot;&quot;
    Implement the backward propagation for a single RELU unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    &quot;&quot;&quot;
    
    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.
    
    # When z &lt;= 0, you should set dz to 0 as well. 
    dZ[Z &lt;= 0] = 0
    
    assert (dZ.shape == Z.shape)
    
    return dZ
</code></pre>

<h4 id="测试函数">测试函数</h4>

<pre><code class="language-python">def predict(X, y, parameters):
    &quot;&quot;&quot;
    This function is used to predict the results of a  L-layer neural network.
    
    Arguments:
    X -- data set of examples you would like to label
    parameters -- parameters of the trained model
    
    Returns:
    p -- predictions for the given dataset X
    &quot;&quot;&quot;
    
    m = X.shape[1]
    n = len(parameters) // 2 # number of layers in the neural network
    p = np.zeros((1,m))
    
    # Forward propagation
    probas, caches = L_model_forward(X, parameters)

    
    # convert probas to 0/1 predictions
    for i in range(0, probas.shape[1]):
        if probas[0,i] &gt; 0.5:
            p[0,i] = 1
        else:
            p[0,i] = 0
    
    #print results
    #print (&quot;predictions: &quot; + str(p))
    #print (&quot;true labels: &quot; + str(y))
    print(&quot;Accuracy: &quot;  + str(np.sum((p == y)/m)))
        
    return p
</code></pre>

<h4 id="数据集载入">数据集载入</h4>

<pre><code class="language-python">def load_data():
    train_dataset = h5py.File('datasets/train_catvnoncat.h5', &quot;r&quot;)
    train_set_x_orig = np.array(train_dataset[&quot;train_set_x&quot;][:]) # your train set features
    train_set_y_orig = np.array(train_dataset[&quot;train_set_y&quot;][:]) # your train set labels

    test_dataset = h5py.File('datasets/test_catvnoncat.h5', &quot;r&quot;)
    test_set_x_orig = np.array(test_dataset[&quot;test_set_x&quot;][:]) # your test set features
    test_set_y_orig = np.array(test_dataset[&quot;test_set_y&quot;][:]) # your test set labels

    classes = np.array(test_dataset[&quot;list_classes&quot;][:]) # the list of classes
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
</code></pre>

<p>有了以上的helper function就可以真正开始构建我们的神经网络了。</p>

<h3 id="前向传播">前向传播</h3>

<h4 id="参数初始化">参数初始化</h4>

<p>想要进行前向传播的计算，首先要对神经元的参数W和b进行初始化，这里我们首先考虑一个简单的2层神经网络，输入层的个数为n_x，隐藏层的个数为n_h，输出层的个数为n_y.
<img src="http://oq393r0ea.bkt.clouddn.com/2layerNN_kiank.png" alt="" />
<center> 图4 2层神经网络</center></p>

<pre><code class="language-python">def initialize_parameters(n_x, n_h, n_y):
    &quot;&quot;&quot;
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    parameters -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    &quot;&quot;&quot;
    
    np.random.seed(1)
    
    W1 = np.random.randn(n_h, n_x)*0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h)*0.01
    b2 = np.zeros((n_y, 1))
    
    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))
    
    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2}
    
    return parameters     
</code></pre>

<p>当我们的网络不止两层时，可以通过传入一个layer_dims的数组指定每层神经网络的神经元个数，使用一个for循环完成参数的初始化。需要注意的是，对于第$l$层的W，它的size为(layer_dims[l],layer_dims[l-1]),b的size为(layer_dims[l],1)。</p>

<pre><code class="language-python">def initialize_parameters_deep(layer_dims):
    &quot;&quot;&quot;
    Arguments:
    layer_dims -- python array (list) containing the dimensions of each layer in our network
    
    Returns:
    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    &quot;&quot;&quot;
    
    np.random.seed(1)
    parameters = {}
    L = len(layer_dims)            # number of layers in the network

    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
        
        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))
        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))

        
    return parameters
</code></pre>

<h4 id="前向传播-1">前向传播</h4>

<p>首先我们进行前向传播的线性部分计算，即$Z=WA+b$.</p>

<pre><code class="language-python">def linear_forward(A, W, b):
    &quot;&quot;&quot;
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter 
    cache -- a python dictionary containing &quot;A&quot;, &quot;W&quot; and &quot;b&quot; ; stored for computing the backward pass efficiently
    &quot;&quot;&quot;
    
    Z = W.dot(A) + b
    
    assert(Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)
    
    return Z, cache
</code></pre>

<p>然后进行第二部分激活函数的计算,这里我们通过activation变量决定是使用sigmoid还是relu激活。函数的返回值除了激活后的A，还有一个cache，cache中第一部分存储了第$l$层神经元的输入$A^{[l-1]}$以及神经元的参数$W^{[l]},b^{[l]}$,第二部分则保存着激活函数的输入$Z^{[l]}$。</p>

<pre><code class="language-python">def linear_activation_forward(A_prev, W, b, activation):
    &quot;&quot;&quot;
    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer

    Arguments:
    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)
    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;

    Returns:
    A -- the output of the activation function, also called the post-activation value 
    cache -- a python dictionary containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;
             stored for computing the backward pass efficiently
    &quot;&quot;&quot;
    
    if activation == &quot;sigmoid&quot;:
        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)
    
    elif activation == &quot;relu&quot;:
        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)
    
    assert (A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache
</code></pre>

<p>以上的代码实现了单层网络的前向传播，对于一个包含多层的深度神经网络，我们可以反复调用以上的代码块进行前向传播，直至得到输出的概率值。以下函数的返回值AL即图片是否有猫的概率值，caches缓存了前向传播过程中的所有中间变量，方便在反向传播计算的时候使用。</p>

<pre><code class="language-python">def L_model_forward(X, parameters):
    &quot;&quot;&quot;
    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation
    
    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialize_parameters_deep()
    
    Returns:
    AL -- last post-activation value
    caches -- list of caches containing:
                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    &quot;&quot;&quot;

    caches = []
    A = X
    L = len(parameters) // 2                  # number of layers in the neural network
    
    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.
    for l in range(1, L):
        A_prev = A 
        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = &quot;relu&quot;)
        caches.append(cache)
    
    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.
    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = &quot;sigmoid&quot;)
    caches.append(cache)
    
    assert(AL.shape == (1,X.shape[1]))
            
    return AL, caches
</code></pre>

<h3 id="计算损失">计算损失</h3>

<p>在本例中，因为涉及的是二分类问题，所以使用交叉熵损失函数。
$$L=-\frac{1}{m}(Ylog(AL)+(1-Y)log(1-AL)) \tag{4}$$</p>

<pre><code class="language-python">def compute_cost(AL, Y):
    &quot;&quot;&quot;
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    &quot;&quot;&quot;
    
    m = Y.shape[1]

    # Compute loss from aL and y.
    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))
    
    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert(cost.shape == ())
    
    return cost
</code></pre>

<h3 id="反向传播">反向传播</h3>

<p>完成了前向传播和损失函数的计算后，我们需要对误差进行反向传播从而得到各个神经元参数相对误差的偏导。
首先我们回顾一下前向传播的公式：
$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \tag{5}$$
对于第$l$层，分别对$W,A,b$求偏导，可得到如下的三个计算式：
$$
dW^{[l]}=\frac{1}{m}dZ^{[l]} \cdot A^{[l-1]T} \<br />
db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdim=True)  \tag{6} \\
dA^{[l-1]}=W^{[l]T} \cdot dZ^{[l]}
$$
根据式(6)，可以得到linear_backward函数。</p>

<pre><code class="language-python">def linear_backward(dZ, cache):
    &quot;&quot;&quot;
    Implement the linear portion of backward propagation for a single layer (layer l)

    Arguments:
    dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    &quot;&quot;&quot;
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1./m * np.dot(dZ,A_prev.T)
    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)
    dA_prev = np.dot(W.T,dZ)
    
    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)
    
    return dA_prev, dW, db
</code></pre>

<p>linear_backward函数传入的参数dZ是该神经元激活前的偏导，作为一个完整的神经元反向传播过程，还应加上对激活函数的反传过程。linear_activation_backward函数的输入值dA即为激活函数前的偏导，同时传入activation参数用于判断是relu还是sigmoid激活的反传。</p>

<pre><code class="language-python">def linear_activation_backward(dA, cache, activation):
    &quot;&quot;&quot;
    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.
    
    Arguments:
    dA -- post-activation gradient for current layer l 
    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;
    
    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    &quot;&quot;&quot;
    linear_cache, activation_cache = cache
    
    if activation == &quot;relu&quot;:
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
        
    elif activation == &quot;sigmoid&quot;:
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    
    return dA_prev, dW, db
</code></pre>

<p>有了单层网络的反向传播函数linear_activation_backward后，通过for循环执行上述函数即可完成对整个网络的反向传播。</p>

<pre><code class="language-python">def L_model_backward(AL, Y, caches):
    &quot;&quot;&quot;
    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group
    
    Arguments:
    AL -- probability vector, output of the forward propagation (L_model_forward())
    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)
    caches -- list of caches containing:
                every cache of linear_activation_forward() with &quot;relu&quot; (there are (L-1) or them, indexes from 0 to L-2)
                the cache of linear_activation_forward() with &quot;sigmoid&quot; (there is one, index L-1)
    
    Returns:
    grads -- A dictionary with the gradients
             grads[&quot;dA&quot; + str(l)] = ... 
             grads[&quot;dW&quot; + str(l)] = ...
             grads[&quot;db&quot; + str(l)] = ... 
    &quot;&quot;&quot;
    grads = {}
    L = len(caches) # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL
    
    # Initializing the backpropagation
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    
    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]
    current_cache = caches[L-1]
    grads[&quot;dA&quot; + str(L)], grads[&quot;dW&quot; + str(L)], grads[&quot;db&quot; + str(L)] = linear_activation_backward(dAL, current_cache, activation = &quot;sigmoid&quot;)
    
    for l in reversed(range(L-1)):
        # lth layer: (RELU -&gt; LINEAR) gradients.
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[&quot;dA&quot; + str(l + 2)], current_cache, activation = &quot;relu&quot;)
        grads[&quot;dA&quot; + str(l + 1)] = dA_prev_temp
        grads[&quot;dW&quot; + str(l + 1)] = dW_temp
        grads[&quot;db&quot; + str(l + 1)] = db_temp

    return grads
</code></pre>

<h3 id="梯度更新-1">梯度更新</h3>

<p>为了简单起见，这里的梯度更新只使用最原始的梯度下降算法。
$$W = W - lr*grad \tag{7}$$</p>

<pre><code class="language-python">def update_parameters(parameters, grads, learning_rate):
    &quot;&quot;&quot;
    Update parameters using gradient descent
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients, output of L_model_backward
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
                  parameters[&quot;W&quot; + str(l)] = ... 
                  parameters[&quot;b&quot; + str(l)] = ...
    &quot;&quot;&quot;
    
    L = len(parameters) // 2 # number of layers in the neural network

    # Update rule for each parameter. Use a for loop.
    for l in range(L):
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)] - learning_rate * grads[&quot;dW&quot; + str(l+1)]
        parameters[&quot;b&quot; + str(l+1)] = parameters[&quot;b&quot; + str(l+1)] - learning_rate * grads[&quot;db&quot; + str(l+1)]
        
    return parameters
</code></pre>

<h2 id="all-to-all">All to all</h2>

<p>ok,我们已经完成了组成一个简单的多层神经网络需要的所有组件，现在我们要做的就是把各个模块组合成一个完整的猫/非猫的图片二分类神经网络。
首先我们先做一些初始化库、数据集的导入工作。</p>

<pre><code class="language-python">import time
import numpy as np
import matplotlib.pyplot as plt
import scipy
from PIL import Image
from scipy import ndimage

%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

np.random.seed(1)

train_x_orig, train_y, test_x_orig, test_y, classes = load_data()
# Reshape the training and test examples 
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The &quot;-1&quot; makes reshape flatten the remaining dimensions
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T

# Standardize data to have feature values between 0 and 1.
train_x = train_x_flatten/255.
test_x = test_x_flatten/255.

print (&quot;train_x's shape: &quot; + str(train_x.shape))
print (&quot;test_x's shape: &quot; + str(test_x.shape))
</code></pre>

<p>将数据集中每个样本归一化并展成列向量后，可以看到如下输出：
train_x&rsquo;s shape: (12288, 209)
test_x&rsquo;s shape: (12288, 50)</p>

<h2 id="两层神经网络">两层神经网络</h2>

<p>首先，让我们构建一个如图4所示的两层神经网络，输入层大小为12288，中间层大小为7，输出层1.</p>

<pre><code class="language-python">### CONSTANTS DEFINING THE MODEL ####
n_x = 12288     # num_px * num_px * 3
n_h = 7
n_y = 1
layers_dims = (n_x, n_h, n_y)

# GRADED FUNCTION: two_layer_model

def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):
    &quot;&quot;&quot;
    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.
    
    Arguments:
    X -- input data, of shape (n_x, number of examples)
    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)
    layers_dims -- dimensions of the layers (n_x, n_h, n_y)
    num_iterations -- number of iterations of the optimization loop
    learning_rate -- learning rate of the gradient descent update rule
    print_cost -- If set to True, this will print the cost every 100 iterations 
    
    Returns:
    parameters -- a dictionary containing W1, W2, b1, and b2
    &quot;&quot;&quot;
    
    np.random.seed(1)
    grads = {}
    costs = []                              # to keep track of the cost
    m = X.shape[1]                           # number of examples
    (n_x, n_h, n_y) = layers_dims
    
    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented
    ### START CODE HERE ### (≈ 1 line of code)
    parameters = initialize_parameters(n_x,n_h,n_y)
    ### END CODE HERE ###
    
    # Get W1, b1, W2 and b2 from the dictionary parameters.
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: &quot;X, W1, b1&quot;. Output: &quot;A1, cache1, A2, cache2&quot;.
        ### START CODE HERE ### (≈ 2 lines of code)
        A1, cache1 = linear_activation_forward(X,W1,b1,&quot;relu&quot;)
        A2, cache2 = linear_activation_forward(A1,W2,b2,&quot;sigmoid&quot;)
        ### END CODE HERE ###
        
        # Compute cost
        ### START CODE HERE ### (≈ 1 line of code)
        cost = compute_cost(A2,Y)
        ### END CODE HERE ###
        
        # Initializing backward propagation
        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))
        
        # Backward propagation. Inputs: &quot;dA2, cache2, cache1&quot;. Outputs: &quot;dA1, dW2, db2; also dA0 (not used), dW1, db1&quot;.
        ### START CODE HERE ### (≈ 2 lines of code)
        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,&quot;sigmoid&quot;)
        dA0, dW1, db1 = linear_activation_backward(dA1,cache1,&quot;relu&quot;)
        ### END CODE HERE ###
        
        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2
        grads['dW1'] = dW1
        grads['db1'] = db1
        grads['dW2'] = dW2
        grads['db2'] = db2
        
        # Update parameters.
        ### START CODE HERE ### (approx. 1 line of code)
        parameters = update_parameters(parameters,grads,learning_rate)
        ### END CODE HERE ###

        # Retrieve W1, b1, W2, b2 from parameters
        W1 = parameters[&quot;W1&quot;]
        b1 = parameters[&quot;b1&quot;]
        W2 = parameters[&quot;W2&quot;]
        b2 = parameters[&quot;b2&quot;]
        
        # Print the cost every 100 training example
        if print_cost and i % 100 == 0:
            print(&quot;Cost after iteration {}: {}&quot;.format(i, np.squeeze(cost)))
        if print_cost and i % 100 == 0:
            costs.append(cost)
       
    # plot the cost

    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per tens)')
    plt.title(&quot;Learning rate =&quot; + str(learning_rate))
    plt.show()
    
    return parameters

parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)
</code></pre>

<p>运行以上的代码，会有如下的输出：
<img src="http://oq393r0ea.bkt.clouddn.com/20170926230204.png" alt="" />
<center>图5 两层网络训练过程</center>
调用predict函数分别计算训练集和测试集的准确率，可以看到在训练集上达到了100%的准确率，而在测试集上准确率为72%。</p>

<pre><code class="language-python">predictions_train = predict(train_x, train_y, parameters)
predictions_test = predict(test_x, test_y, parameters)
print(predictions_train)
print(predictions_test)
</code></pre>

<h2 id="l层神经网络">L层神经网络</h2>

<p>在本文的最后部分，我们会利用前面的代码构建一个L层的神经网络，前L-1层网络均使用relu激活，最后一层使用sigmoid激活。这里假设L=5.</p>

<pre><code class="language-python">### CONSTANTS ###
layers_dims = [12288, 20, 7, 5, 1] #  5-layer model

# GRADED FUNCTION: L_layer_model

def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009
    &quot;&quot;&quot;
    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.
    
    Arguments:
    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)
    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)
    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).
    learning_rate -- learning rate of the gradient descent update rule
    num_iterations -- number of iterations of the optimization loop
    print_cost -- if True, it prints the cost every 100 steps
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;

    np.random.seed(1)
    costs = []                         # keep track of cost
    
    # Parameters initialization.
    ### START CODE HERE ###
    parameters = initialize_parameters_deep(layers_dims)
    ### END CODE HERE ###
    
    # Loop (gradient descent)
    for i in range(0, num_iterations):

        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.
        ### START CODE HERE ### (≈ 1 line of code)
        AL, caches = L_model_forward(X,parameters)
        ### END CODE HERE ###
        
        # Compute cost.
        ### START CODE HERE ### (≈ 1 line of code)
        cost = compute_cost(AL,Y)
        ### END CODE HERE ###
    
        # Backward propagation.
        ### START CODE HERE ### (≈ 1 line of code)
        grads = L_model_backward(AL,Y,caches)
        ### END CODE HERE ###
 
        # Update parameters.
        ### START CODE HERE ### (≈ 1 line of code)
        parameters = update_parameters(parameters,grads,learning_rate)
        ### END CODE HERE ###
                
        # Print the cost every 100 training example
        if print_cost and i % 100 == 0:
            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)
            
    # plot the cost
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per tens)')
    plt.title(&quot;Learning rate =&quot; + str(learning_rate))
    plt.show()
    
    return parameters

parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)
</code></pre>

<p>运行以上的代码，会有如下的输出：
<img src="http://oq393r0ea.bkt.clouddn.com/20170926232251.png" alt="" />
<center>图6 L层网路训练过程</center>
同样，我们使用predict函数看看训练集和测试集的准确率。</p>

<pre><code class="language-python">pred_train = predict(train_x, train_y, parameters)
pred_test = predict(test_x, test_y, parameters)
</code></pre>

<p>此时，当网络加深后，测试集的准确率也从先前的72%提高到了80%.</p>

<p>如果你希望看到完整的代码示例，可以访问我的<a href="https://github.com/BoyuanJiang/DNN_Step_by_Step" target="_blank">github</a></p>

<hr />

<h2 id="参考文献">参考文献</h2>

<ol>
<li><a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome" target="_blank">Neural Networks and Deep Learning</a></li>
</ol>
      </div>

      




    </div>
  </div>

</article>






<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js"></script>
    <script src="/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      
      <script src="/libs/highlight.js/9.12.0/highlight.min.js"></script>
      

      
      <script src="https://byjiang.com/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
    
    

  </body>
</html>

